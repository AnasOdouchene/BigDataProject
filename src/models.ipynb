{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b31ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Données Initiales =====\n",
      "+--------+-----------+----------+-----------+-------------+-----------+--------------------+----------+-------------------+------+-------------+\n",
      "|      ID|Initialdate| Finaldate|    Area_ha|      Area_m2|   Area_Km2|         CountryName| Continent|             Region|Season|Duration_days|\n",
      "+--------+-----------+----------+-----------+-------------+-----------+--------------------+----------+-------------------+------+-------------+\n",
      "|25078590| 2022-01-09|2022-02-06|50232.10763|5.023210763E8|502.3210763|               Ghana|    Africa|     Western Africa|Winter|           28|\n",
      "|25079092| 2022-01-11|2022-02-08|82380.29538|8.238029538E8|823.8029538|             Nigeria|    Africa|     Western Africa|Winter|           28|\n",
      "|25079113| 2022-01-11|2022-02-03|36851.12748|3.685112748E8|368.5112748|             Nigeria|    Africa|     Western Africa|Winter|           23|\n",
      "|25083241| 2022-01-03|2022-02-12|43303.63519|4.330363519E8|433.0363519|             Nigeria|    Africa|     Western Africa|Winter|           40|\n",
      "|25095507| 2022-01-01|2022-02-11|75753.14059|7.575314059E8|757.5314059|   Central Africa...|    Africa|      Middle Africa|Winter|           41|\n",
      "+--------+-----------+----------+-----------+-------------+-----------+--------------------+----------+-------------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col , when\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pickle\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PredictionSession\").config(\"spark.local.dir\", \"C:/temp\").config(\"spark.executor.memory\", \"4g\").config(\"spark.driver.memory\", \"4g\").getOrCreate()\n",
    "\n",
    "dataset_path = \"../data/Transformed_GlobalFireBurnedArea_pandas.csv\"\n",
    "df = spark.read.csv(dataset_path, header=True, inferSchema=True)\n",
    "\n",
    "# Afficher les 5 premières lignes des données initiales\n",
    "print(\"===== Données Initiales =====\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10e5579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|severity| count|\n",
      "+--------+------+\n",
      "|  Medium|  6668|\n",
      "|     Low|312489|\n",
      "|    High|   121|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"severity\",\n",
    "    when(col(\"Area_Km2\") < 50, \"Low\")\n",
    "    .when((col(\"Area_Km2\") >= 50) & (col(\"Area_Km2\") < 1000), \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "# Compter le nombre d'incendies par catégorie\n",
    "df.groupBy(\"severity\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "637ff5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Après encodage des colonnes catégoriques =====\n",
      "+--------+--------------+------+------------+\n",
      "|severity|severity_index|Season|Season_index|\n",
      "+--------+--------------+------+------------+\n",
      "|  Medium|           1.0|Winter|         2.0|\n",
      "|  Medium|           1.0|Winter|         2.0|\n",
      "|  Medium|           1.0|Winter|         2.0|\n",
      "|  Medium|           1.0|Winter|         2.0|\n",
      "|  Medium|           1.0|Winter|         2.0|\n",
      "+--------+--------------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(\n",
    "    inputCols=[\"severity\", \"Season\", \"CountryName\", \"Region\", \"Continent\"],\n",
    "    outputCols=[\"severity_index\", \"Season_index\", \"CountryName_index\", \"Region_index\", \"Continent_index\"]\n",
    ")\n",
    "df = indexer.fit(df).transform(df)\n",
    "print(\"===== Après encodage des colonnes catégoriques =====\")\n",
    "df.select(\"severity\", \"severity_index\", \"Season\", \"Season_index\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e16a2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Season| count|\n",
      "+------+------+\n",
      "|Winter| 79605|\n",
      "|Spring| 50249|\n",
      "|Summer|108957|\n",
      "|Autumn| 80467|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Season\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c770afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schéma après gestion des valeurs nulles :\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Initialdate: date (nullable = true)\n",
      " |-- Finaldate: date (nullable = true)\n",
      " |-- Area_ha: double (nullable = true)\n",
      " |-- Area_m2: double (nullable = true)\n",
      " |-- Area_Km2: double (nullable = true)\n",
      " |-- CountryName: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Duration_days: double (nullable = true)\n",
      " |-- severity: string (nullable = false)\n",
      " |-- severity_index: double (nullable = false)\n",
      " |-- Season_index: double (nullable = false)\n",
      " |-- CountryName_index: double (nullable = false)\n",
      " |-- Region_index: double (nullable = false)\n",
      " |-- Continent_index: double (nullable = false)\n",
      "\n",
      "===== Après assemblage des caractéristiques =====\n",
      "+-----------------------+--------------+-----------+\n",
      "|features               |severity_index|Area_Km2   |\n",
      "+-----------------------+--------------+-----------+\n",
      "|[2.0,28.0,17.0,2.0,0.0]|1.0           |502.3210763|\n",
      "|[2.0,28.0,9.0,2.0,0.0] |1.0           |823.8029538|\n",
      "|[2.0,23.0,9.0,2.0,0.0] |1.0           |368.5112748|\n",
      "|[2.0,40.0,9.0,2.0,0.0] |1.0           |433.0363519|\n",
      "|[2.0,41.0,5.0,0.0,0.0] |1.0           |757.5314059|\n",
      "+-----------------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Vérifier et gérer les valeurs nulles dans les colonnes\n",
    "df = df.withColumn(\"Season_index\", when(col(\"Season_index\").isNull(), 0).otherwise(col(\"Season_index\")))\n",
    "df = df.withColumn(\"Duration_days\", when(col(\"Duration_days\").isNull(), 0).otherwise(col(\"Duration_days\").cast(\"double\")))\n",
    "df = df.withColumn(\"CountryName_index\", when(col(\"CountryName_index\").isNull(), 0).otherwise(col(\"CountryName_index\")))\n",
    "df = df.withColumn(\"Region_index\", when(col(\"Region_index\").isNull(), 0).otherwise(col(\"Region_index\")))\n",
    "df = df.withColumn(\"Continent_index\", when(col(\"Continent_index\").isNull(), 0).otherwise(col(\"Continent_index\")))\n",
    "\n",
    "# Vérifier le schéma après gestion des valeurs nulles\n",
    "print(\"Schéma après gestion des valeurs nulles :\")\n",
    "df.printSchema()\n",
    "\n",
    "# Définir les colonnes d'entrée pour VectorAssembler\n",
    "feature_cols = [\"Season_index\", \"Duration_days\", \"CountryName_index\", \"Region_index\", \"Continent_index\"]\n",
    "\n",
    "# Initialiser VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Appliquer la transformation\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"===== Après assemblage des caractéristiques =====\")\n",
    "df.select(\"features\", \"severity_index\", \"Area_Km2\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efc4b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Données d'entraînement =====\n",
      "+--------+-----------+----------+-----------+-------------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+------------------------+\n",
      "|ID      |Initialdate|Finaldate |Area_ha    |Area_m2      |Area_Km2   |CountryName                |Continent   |Region               |Season|Duration_days|severity|severity_index|Season_index|CountryName_index|Region_index|Continent_index|features                |\n",
      "+--------+-----------+----------+-----------+-------------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+------------------------+\n",
      "|25036465|2022-01-09 |2022-01-14|214.8744509|2148744.509  |2.148744509|   United States of America|    Americas|     Northern America|Winter|5.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,5.0,22.0,10.0,1.0] |\n",
      "|25036468|2022-01-04 |2022-01-19|1439.054333|1.439054333E7|14.39054333|   United States of America|    Americas|     Northern America|Winter|15.0         |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,15.0,22.0,10.0,1.0]|\n",
      "|25036474|2022-01-07 |2022-01-09|107.3784157|1073784.157  |1.073784157|   United States of America|    Americas|     Northern America|Winter|2.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,2.0,22.0,10.0,1.0] |\n",
      "|25036477|2022-01-07 |2022-01-12|493.939082 |4939390.82   |4.93939082 |   United States of America|    Americas|     Northern America|Winter|5.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,5.0,22.0,10.0,1.0] |\n",
      "|25036478|2022-01-07 |2022-01-10|107.3770202|1073770.202  |1.073770202|   United States of America|    Americas|     Northern America|Winter|3.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,3.0,22.0,10.0,1.0] |\n",
      "+--------+-----------+----------+-----------+-------------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "===== Données de test =====\n",
      "+--------+-----------+----------+-----------+-----------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+-----------------------+\n",
      "|ID      |Initialdate|Finaldate |Area_ha    |Area_m2    |Area_Km2   |CountryName                |Continent   |Region               |Season|Duration_days|severity|severity_index|Season_index|CountryName_index|Region_index|Continent_index|features               |\n",
      "+--------+-----------+----------+-----------+-----------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+-----------------------+\n",
      "|25036472|2022-01-03 |2022-01-07|107.3939977|1073939.977|1.073939977|   United States of America|    Americas|     Northern America|Winter|4.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,4.0,22.0,10.0,1.0]|\n",
      "|25036483|2022-01-07 |2022-01-12|322.1117395|3221117.395|3.221117395|   United States of America|    Americas|     Northern America|Winter|5.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,5.0,22.0,10.0,1.0]|\n",
      "|25036489|2022-01-09 |2022-01-15|193.1822245|1931822.245|1.931822245|   United States of America|    Americas|     Northern America|Winter|6.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,6.0,22.0,10.0,1.0]|\n",
      "|25036500|2022-01-03 |2022-01-06|107.226946 |1072269.46 |1.07226946 |   Mexico                  |    Americas|     Central America |Winter|3.0          |Low     |0.0           |2.0         |33.0             |11.0        |1.0            |[2.0,3.0,33.0,11.0,1.0]|\n",
      "|25036515|2022-01-19 |2022-01-20|128.670946 |1286709.46 |1.28670946 |   Mexico                  |    Americas|     Central America |Winter|1.0          |Low     |0.0           |2.0         |33.0             |11.0        |1.0            |[2.0,1.0,33.0,11.0,1.0]|\n",
      "+--------+-----------+----------+-----------+-----------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"===== Données d'entraînement =====\")\n",
    "train_data.show(5, truncate=False)\n",
    "print(\"===== Données de test =====\")\n",
    "test_data.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4cd0115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Objectif 1 : Prédiction de la gravité =====\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Entraîner le modèle de classification\n",
    "print(\"===== Objectif 1 : Prédiction de la gravité =====\")\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"severity_index\", featuresCol=\"features\",maxBins=150)\n",
    "rf_model = rf_classifier.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9766664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle de classification a été sauvegardé sous 'rf_model_classification'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Créer un dossier pour sauvegarder les modèles (si ce n'est pas déjà fait)\n",
    "model_save_dir = 'modele_sauvegarder'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarder le modèle Spark ML\n",
    "rf_model.write().overwrite().save(os.path.join(model_save_dir, 'rf_model_classification'))\n",
    "print(\"Le modèle de classification a été sauvegardé sous 'rf_model_classification'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26ec811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Prédictions pour la classification =====\n",
      "+--------+--------------+----------+--------------------+\n",
      "|severity|severity_index|prediction|         probability|\n",
      "+--------+--------------+----------+--------------------+\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "+--------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Précision du modèle de classification : 0.98\n"
     ]
    }
   ],
   "source": [
    "# Faire des prédictions\n",
    "predictions_classifier = rf_model.transform(test_data)\n",
    "print(\"===== Prédictions pour la classification =====\")\n",
    "predictions_classifier.select(\"severity\", \"severity_index\", \"prediction\", \"probability\").show(5)\n",
    "\n",
    "# Évaluer les performances\n",
    "classification_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"severity_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = classification_evaluator.evaluate(predictions_classifier)\n",
    "print(f\"Précision du modèle de classification : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9403aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from modele_sauvegarder/rf_model_classification\n",
      "===== Test Data =====\n",
      "+--------+-----------+----------+-----------+-----------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+-----------------------+\n",
      "|ID      |Initialdate|Finaldate |Area_ha    |Area_m2    |Area_Km2   |CountryName                |Continent   |Region               |Season|Duration_days|severity|severity_index|Season_index|CountryName_index|Region_index|Continent_index|features               |\n",
      "+--------+-----------+----------+-----------+-----------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+-----------------------+\n",
      "|25036472|2022-01-03 |2022-01-07|107.3939977|1073939.977|1.073939977|   United States of America|    Americas|     Northern America|Winter|4.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,4.0,22.0,10.0,1.0]|\n",
      "|25036483|2022-01-07 |2022-01-12|322.1117395|3221117.395|3.221117395|   United States of America|    Americas|     Northern America|Winter|5.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,5.0,22.0,10.0,1.0]|\n",
      "|25036489|2022-01-09 |2022-01-15|193.1822245|1931822.245|1.931822245|   United States of America|    Americas|     Northern America|Winter|6.0          |Low     |0.0           |2.0         |22.0             |10.0        |1.0            |[2.0,6.0,22.0,10.0,1.0]|\n",
      "|25036500|2022-01-03 |2022-01-06|107.226946 |1072269.46 |1.07226946 |   Mexico                  |    Americas|     Central America |Winter|3.0          |Low     |0.0           |2.0         |33.0             |11.0        |1.0            |[2.0,3.0,33.0,11.0,1.0]|\n",
      "|25036515|2022-01-19 |2022-01-20|128.670946 |1286709.46 |1.28670946 |   Mexico                  |    Americas|     Central America |Winter|1.0          |Low     |0.0           |2.0         |33.0             |11.0        |1.0            |[2.0,1.0,33.0,11.0,1.0]|\n",
      "+--------+-----------+----------+-----------+-----------+-----------+---------------------------+------------+---------------------+------+-------------+--------+--------------+------------+-----------------+------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "===== Predictions for Classification =====\n",
      "+--------+--------------+----------+--------------------+\n",
      "|severity|severity_index|prediction|         probability|\n",
      "+--------+--------------+----------+--------------------+\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "|     Low|           0.0|       0.0|[0.98794056009154...|\n",
      "+--------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Predictions have been sent to Kafka topic.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# Patch kafka-python for Python 3.12 and later versions\n",
    "if sys.version_info >= (3, 12, 0):\n",
    "    import six\n",
    "    sys.modules['kafka.vendor.six.moves'] = six.moves\n",
    "\n",
    "# 1. Load the saved ML model\n",
    "model_save_dir = 'modele_sauvegarder'\n",
    "rf_model_path = f\"{model_save_dir}/rf_model_classification\"\n",
    "rf_model = RandomForestClassificationModel.load(rf_model_path)\n",
    "print(f\"Model loaded from {rf_model_path}\")\n",
    "\n",
    "# 2. Use `test_data` defined earlier in the notebook (already split)\n",
    "print(\"===== Test Data =====\")\n",
    "test_data.show(5, truncate=False)\n",
    "\n",
    "# 3. Make predictions on the test data\n",
    "predictions_classifier = rf_model.transform(test_data)\n",
    "print(\"===== Predictions for Classification =====\")\n",
    "predictions_classifier.select(\"severity\", \"severity_index\", \"prediction\", \"probability\").show(5)\n",
    "\n",
    "# 4. Configure Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "# 5. Send predictions to the Kafka topic\n",
    "for row in predictions_classifier.collect():\n",
    "    message = {\n",
    "        'severity': row['severity'],\n",
    "        'severity_index': row['severity_index'],\n",
    "        'prediction': row['prediction'],\n",
    "        'probability': row['probability'].toArray().tolist()  # Convert the vector to a list\n",
    "    }\n",
    "    producer.send('severity-predictions', value=message)\n",
    "\n",
    "# 6. Close the Producer after sending the messages\n",
    "producer.flush()\n",
    "producer.close()\n",
    "print(\"Predictions have been sent to Kafka topic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f4636e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture des messages depuis le topic 'severity-predictions'...\n",
      "Données reçues : {'severity': 'Low', 'severity_index': 0.0, 'prediction': 0.0, 'probability': [0.9880936249292329, 0.011848084557354189, 5.829051341284061e-05]}\n",
      "Données reçues : {'severity': 'Low', 'severity_index': 0.0, 'prediction': 0.0, 'probability': [0.9880936249292329, 0.011848084557354189, 5.829051341284061e-05]}\n",
      "Données reçues : {'severity': 'Low', 'severity_index': 0.0, 'prediction': 0.0, 'probability': [0.9880936249292329, 0.011848084557354189, 5.829051341284061e-05]}\n",
      "Données reçues : {'severity': 'Low', 'severity_index': 0.0, 'prediction': 0.0, 'probability': [0.9880936249292329, 0.011848084557354189, 5.829051341284061e-05]}\n",
      "Données reçues : {'severity': 'Low', 'severity_index': 0.0, 'prediction': 0.0, 'probability': [0.9880936249292329, 0.011848084557354189, 5.829051341284061e-05]}\n",
      "Limite de 5 messages atteinte. Arrêt de la consommation.\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Configurer le Kafka Consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'severity-predictions',  # Nom du topic\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "    enable_auto_commit=True,\n",
    "    auto_offset_reset='earliest'  # Lire les messages depuis le début\n",
    ")\n",
    "\n",
    "# Limite des lignes à lire\n",
    "max_rows = 5\n",
    "count = 0\n",
    "\n",
    "print(\"Lecture des messages depuis le topic 'severity-predictions'...\")\n",
    "\n",
    "# Lire les messages depuis Kafka en boucle\n",
    "while count < max_rows:\n",
    "    # Poller les messages (timeout de 1 seconde)\n",
    "    messages = consumer.poll(timeout_ms=1000, max_records=5)\n",
    "\n",
    "    # Vérifier s'il y a des messages\n",
    "    if messages:\n",
    "        for topic_partition, msgs in messages.items():\n",
    "            for message in msgs:\n",
    "                # Afficher les données reçues\n",
    "                print(f\"Données reçues : {message.value}\")\n",
    "\n",
    "                # Incrémenter le compteur\n",
    "                count += 1\n",
    "\n",
    "                # Vérifier si la limite est atteinte\n",
    "                if count >= max_rows:\n",
    "                    print(f\"Limite de {max_rows} messages atteinte. Arrêt de la consommation.\")\n",
    "                    break\n",
    "    else:\n",
    "        print(\"Aucun message reçu, réessayer...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2269d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|severity_index|count|\n",
      "+--------------+-----+\n",
      "|           0.0|62212|\n",
      "|           1.0| 1335|\n",
      "|           2.0|   24|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.groupBy(\"severity_index\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df109e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Objectif 2 : Prédiction de la taille =====\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 1. Entraîner le modèle de régression\n",
    "# ---------------------\n",
    "print(\"===== Objectif 2 : Prédiction de la taille =====\")\n",
    "\n",
    "# Modèle de RandomForestRegressor avec paramètres optimisés\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    labelCol=\"Area_Km2\",    # Colonne cible\n",
    "    featuresCol=\"features\", # Colonne des caractéristiques\n",
    "    maxBins=150,            # Nombre maximum de bins (important pour les variables catégoriques)\n",
    "    numTrees=100,           # Nombre d'arbres\n",
    "    maxDepth=10             # Profondeur maximale des arbres\n",
    ")\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "rf_model_regressor = rf_regressor.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c9d1f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle de régression a été sauvegardé sous 'rf_model_regression'.\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle Spark ML\n",
    "rf_model_regressor.write().overwrite().save(os.path.join(model_save_dir, 'rf_model_regression'))\n",
    "print(\"Le modèle de régression a été sauvegardé sous 'rf_model_regression'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c89e4ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Prédictions pour la régression =====\n",
      "+-----------+------------------+\n",
      "|Area_Km2   |prediction        |\n",
      "+-----------+------------------+\n",
      "|1.073939977|4.109420932189103 |\n",
      "|3.221117395|4.488870615041736 |\n",
      "|1.931822245|4.747800911858084 |\n",
      "|1.07226946 |3.7202377130998827|\n",
      "|1.28670946 |3.583961881666361 |\n",
      "|2.78783539 |4.990263400145997 |\n",
      "|7.290904512|5.30571257535233  |\n",
      "|3.429793055|3.772919339617944 |\n",
      "|3.643775961|4.990263400145997 |\n",
      "|2.572134754|5.983122490676127 |\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "===== Évaluation des performances =====\n",
      "RMSE (Root Mean Squared Error) : 46.98\n",
      "MAE (Mean Absolute Error) : 6.84\n",
      "===== Importance des caractéristiques =====\n",
      "Season_index: 0.09\n",
      "Duration_days: 0.69\n",
      "CountryName_index: 0.15\n",
      "Region_index: 0.06\n",
      "Continent_index: 0.01\n"
     ]
    }
   ],
   "source": [
    "# 2. Faire des prédictions\n",
    "# ---------------------\n",
    "predictions_regressor = rf_model_regressor.transform(test_data)\n",
    "\n",
    "# Afficher un échantillon des prédictions\n",
    "print(\"===== Prédictions pour la régression =====\")\n",
    "predictions_regressor.select(\"Area_Km2\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# ---------------------\n",
    "# 3. Évaluer les performances\n",
    "# ---------------------\n",
    "# Initialiser l'évaluateur pour RMSE et MAE\n",
    "regression_evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"Area_Km2\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ")\n",
    "regression_evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"Area_Km2\", predictionCol=\"prediction\", metricName=\"mae\"\n",
    ")\n",
    "\n",
    "# Calculer RMSE et MAE\n",
    "rmse = regression_evaluator_rmse.evaluate(predictions_regressor)\n",
    "mae = regression_evaluator_mae.evaluate(predictions_regressor)\n",
    "\n",
    "# Afficher les métriques d'évaluation\n",
    "print(\"===== Évaluation des performances =====\")\n",
    "print(f\"RMSE (Root Mean Squared Error) : {rmse:.2f}\")\n",
    "print(f\"MAE (Mean Absolute Error) : {mae:.2f}\")\n",
    "\n",
    "# ---------------------\n",
    "# 4. Importance des caractéristiques\n",
    "# ---------------------\n",
    "print(\"===== Importance des caractéristiques =====\")\n",
    "feature_importances = rf_model_regressor.featureImportances\n",
    "for col, importance in zip(feature_cols, feature_importances):\n",
    "    print(f\"{col}: {importance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53959382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|   Area_Km2|        prediction|\n",
      "+-----------+------------------+\n",
      "|1.073939977| 4.109420932189103|\n",
      "|3.221117395| 4.488870615041736|\n",
      "|1.931822245| 4.747800911858084|\n",
      "| 1.07226946|3.7202377130998827|\n",
      "| 1.28670946| 3.583961881666361|\n",
      "| 2.78783539| 4.990263400145997|\n",
      "|7.290904512|  5.30571257535233|\n",
      "|3.429793055| 3.772919339617944|\n",
      "|3.643775961| 4.990263400145997|\n",
      "|2.572134754| 5.983122490676127|\n",
      "|2.357561481|3.9527324413517175|\n",
      "|1.500145118|  3.58047076672232|\n",
      "|1.928687191|  3.58047076672232|\n",
      "|3.213587668| 4.196112729722651|\n",
      "|1.284796405|3.7202377130998827|\n",
      "| 2.35506137|3.5998953442460864|\n",
      "|1.283970857| 3.772919339617944|\n",
      "|1.069900176|3.7202377130998827|\n",
      "|3.867095267| 5.313044049731388|\n",
      "|1.502286694| 3.855159253437626|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_regressor.select(\"Area_Km2\", \"prediction\").show(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
